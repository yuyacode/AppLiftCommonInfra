# 設計に関する意思決定の記録 (Architecture Decision Records)

このドキュメントは、本プロジェクトにおける設計や技術面での様々な意思決定に関する記録です。

---

**単一VPC構成を採用する理由**
- DBアクセスの集約とセキュリティ確保
    - アプリケーションを構成する「LaravelのFargateタスク」と「Go APIのFargateタスク」は、同一RDSを参照, 操作する
    - このDBへのアクセス経路をインターネット経由にせず、プライベートなネットワーク内に限定することでセキュリティが向上する
- マルチVPC構成のデメリット回避
    - 仮にサービスごとにVPCを分割するマルチVPC構成を検討した場合、「設計の複雑化とコスト増」という課題が発生する
        - VPC PeeringやAWS PrivateLinkを利用してVPC間をプライベートに接続する方法もある。しかし、この方法は設計の複雑性を高め、導入, 運用コストも増加させる
        - 本アプリケーション規模や要件において、これらのコストをかけてまでVPCを分割するメリットは特に見出せない
- サービス間通信の効率化
    - 本システムでは、以下のようにサービス間の内部通信が頻繁に発生する
        - Laravel →（内部ALB）→ Go API
        - Lambda →（内部ALB）→ Go API
    - 単一VPC構成にすることで、これらの通信がすべてVPC内で完結し、シンプルで低レイテンシな通信を実現できる
- プロジェクトの規模
    - 大規模な組織でシステム間の境界を明確に分離するためにVPCを分けるケースもあるが、本アプリケーションの規模ではその必要性がなく、単一VPCにリソースを集約する設計が合理的と判断する
- 単一VPC構成がもたらすメリットの総括
    - プライベートIPによる直接通信：パブリックIPを介さずセキュアに直接通信できる
    - 厳密なアクセスコントロール：セキュリティグループを用いて、リソース間の通信を明示的に許可,拒否でき、最小権限の原則を徹底できる。
    - 低レイテンシ：ネットワーク経路がVPC内で完結するため、通信の遅延を最小限に抑えられる
    - シンプルな運用管理：VPC PeeringやTransit Gatewayといった追加コンポーネントの管理が不要となり、インフラ全体の構成がシンプルで理解しやすくなる
- 以上の理由から、本プロジェクトでは単一VPC構成が最もシンプルかつセキュアで、コスト効率の良い最適な選択肢であると結論付ける

<br>

**EKSではなくECSを選択した理由**
- 学習コストと運用負荷の低さ
    - EKS（Kubernetes）の特性
        - 非常に高い柔軟性を持つ反面、その習得と運用には広範な知識（Kubernetesの基本概念、マニフェストファイルの管理、コントロールプレーンのアーキテクチャ理解など）が要求され、学習コストが大きい
    - ECSの特性
        - AWS独自のサービスであり、設定項目や運用手順がKubernetesに比べてシンプルに設計されているため、導入・運用が比較的容易である
- AWSサービスとの統合のシンプルさ
    - ECSの利点
        - ALB, IAM, VPC, CloudWatchといった他のAWSネイティブサービスとの連携が、シームレスに行えるよう標準で設計されている
    - EKSとの比較
        - EKSでもこれらのサービス連携は可能であるが、多くの場合、追加の設定やKubernetes特有のマニフェストによる管理が必要となる
- コントロールプレーンがフルマネージドであること
    - EKSの場合
        - 利用者はKubernetesコントロールプレーン自体のバージョン管理やアップグレード計画を考慮する必要がある
    - ECSの場合
        - クラスターという概念は存在するものの、コントロールプレーンはAWSによって完全に管理されるフルマネージドサービスであるため、利用者がそのバージョン管理を意識する必要は基本的にない
- 十分なスケーラビリティと機能
    - 小〜中規模のアプリケーションを運用する上で、ECSは必要十分なスケーリング機能やオーケストレーション機能を提供する
    - 「特定のKubernetesプラグインを利用したい」「オンプレミス環境とクラウドでKubernetesという統一基盤を使いたい」といった特段の要件がない限り、ECSで機能的に問題となるケースは少ないと判断する

<br>

**EC2ではなくFargateを選択した理由**
- サーバーレスな運用によるインフラ管理コストの削減
    - FargateはEC2インスタンスのプロビジョニング、OSのパッチ適用、セキュリティアップデートといったサーバー管理を意識せずにコンテナを実行できる
    - EC2を利用する場合、インスタンスタイプの選定、AMIの管理、Auto Scalingの複雑な設定、OSレベルのアップデート対応などが必要となる
- 運用のシンプル化
    - ECS on EC2では、クラスター内のEC2インスタンス容量管理（タスクを配置するための余剰CPU/メモリの把握、スケールイン/アウト制御）が発生する
    - Fargateはタスク定義で指定されたリソースに基づき実行環境を自動で確保するため、クラスターの容量計画やスケール制御が不要になる
- セキュリティメリット
    - FargateはAWSが管理する専用基盤上でコンテナを実行し、タスクごとにカーネルレベルで分離されたサンドボックス環境を提供する
    - EC2では、ホストOS上に複数のコンテナが同居するため、ホスト自体のセキュリティ管理（OSパッチ、Dockerデーモンの脆弱性対応など）も利用者の責任範囲となる
- スケーリング動作の簡潔さ
    - オートスケーリングは「必要な分だけFargateタスクを増減させる」という直感的な形で実現できる
    - ECS on EC2では、コンテナ（タスク）のスケーリングとホスト（EC2インスタンス）のスケーリングを両方管理する必要があり、設定がより複雑になる

<br>

**Go APIへの通信にALBを導入する理由**
- ECSサービスのオートスケーリングへの対応
    - Go APIが稼働するECSサービスでは、負荷に応じてタスク数を自動で増減させるオートスケーリングを設定している
    - 内部ALBは、このオートスケーリングによって動的に起動・停止されるタスクを自動で追従し、トラフィックを適切に分散させるために必須となる
- 動的に増減するタスクの自動検出とヘルスチェック
    - スケールアウトによってタスクが増加すると、そのプライベートIPアドレスは動的に割り当てられる
    - ALBはこれらの新しいタスクをターゲットグループに自動で登録し、ヘルスチェックを実行する
    - ヘルスチェックを通過した健全なタスクにのみリクエストをルーティングし、異常なタスクは自動で切り離すことで、サービス全体の信頼性を担保する
- アクセス先の一元化
    - LaravelやLambdaといった呼び出し元のサービスは、背後でタスク数がどのように変動しても、ALBの固定DNS名にアクセスするだけで済む
- Service Discoveryだけでは不十分なケースへの対応
    - ECS Service Discovery機能だけでは、オートスケーリングされたタスク群に対する高度なヘルスチェックや均等な負荷分散が難しいため、ALBを組み合わせる構成が最適である

<br>

**ライフサイクルと責務に応じたスタック分割の採用理由**
- 更新の影響範囲の局所化
    - 単一の巨大なスタックでは、軽微な変更でもCloudFormationが全リソースの差分チェックを行うため、意図しないリソースへの影響リスクや更新時間の増大を招く
    - スタックを分割することで、更新対象を特定のスタックに限定できる（例：ネットワークに変更がなければネットワークスタックは更新しない）
    - 更新失敗時のロールバックもそのスタック内に閉じるため、影響範囲を最小限に抑えることができ、システム全体の安定性が向上する
- 管理性と保守性の向上
    - テンプレートの肥大化防止
        - 分割されたテンプレートは記述量が少なく、構造の把握が容易になるため、可読性とメンテナンス性が向上する
    - デプロイの高速化
        - スタックあたりのリソース数が減ることで、変更セットの作成や更新実行にかかる時間が短縮される
    - トラブルシューティングの効率化
        - 問題が発生した際に、原因究明の対象となるリソース範囲が狭まるため、迅速な対応が可能になる
- ライフサイクルに基づいたリソース管理
    - リソースはそれぞれ変更頻度（ライフサイクル）が異なる（例：ネットワークは構築後ほぼ変更されないが、アプリケーションは頻繁にデプロイされる）
    - ライフサイクルが異なるリソースを同一スタックで管理すると、変更が不要なリソースまで更新の対象となり、リスクが増加する
    - 変更頻度に応じてスタックを分割することで、各リソースをそのライフサイクルに合わせた適切な単位で管理できる

<br>

**GitHub ActionsのAWS認証にOIDCを採用する理由**
- 静的アクセスキー（GitHub Secrets）を利用する方式
    - 概要
        - IAMユーザーのアクセスキーを発行し、GitHubリポジトリのSecretsに保存する方法
    - メリット
        - 手順が直感的で導入が容易である
    - デメリット
        - 長期的なキーの固定化：Secretsに保存されたキーが万が一漏洩した場合、ローテーションされるまで永続的に悪用されるリスクがある
        - キーローテーションの運用負荷：セキュリティ維持のために定期的なキーのローテーション作業が必要となり、手動または自動化のコストが発生する
        - 煩雑な権限管理：リポジトリや環境ごとにキーを発行すると、IAMユーザーの管理が複雑化する
- OIDC（OpenID Connect）を利用する方式
    - 概要
        - GitHub Actionsのワークフロー実行時に、AWSから直接一時的な認証情報を取得する方法
    - メリット
        - キーレス認証の実現：GitHub Secretsに永続的なキーを保存する必要がなく、漏洩リスクを根本から排除できる
        - キーローテーションが不要：認証は都度発行される一時的なセッショントークンで行われるため、キーローテーションの運用が不要になる
        - 詳細な権限制御：AWSのIAMロールポリシーで「特定リポジトリの特定ブランチからの要求のみを許可する」といった、きめ細かなアクセスコントロールが可能になる
- 採用理由
    - セキュリティの優位性
        - 静的な認証情報をリポジトリに保存しないOIDC方式は、アクセスキー方式に比べて圧倒的にセキュアである
    - 運用負荷の軽減
        - 導入後のキー管理やローテーションといった運用作業が不要になるため、長期的な運用コストを削減できる
    - 現代のベストプラクティス
        - AWSおよびGitHubの公式ドキュメントで強く推奨されている標準的な認証方式である
    - 結論
        - 初回設定のわずかな手間を考慮しても、セキュリティリスクの低減と運用負荷の軽減というメリットが大きいため、OIDCを採用する

<br>

**デプロイをジョブ単位で分割する理由**
- 柔軟な実行順序の制御と並列化
    - 各デプロイプロセスをジョブとして独立させることで、needsキーワードを用いて明確な依存関係を定義できる（例: deploy-dbはdeploy-vpcの完了後に実行）
    - 依存関係のないジョブ同士は並列実行が可能となり、デプロイ全体の時間を短縮できる
- ジョブ単位での環境と権限の分離
    - ジョブごとに異なる実行環境（ランナー）やIAMロールを指定できる
        - 例えば「ネットワーク更新には管理者権限のロール」「アプリケーション更新には限定的な権限のロール」といった、セキュリティ上望ましい権限分離が容易に実現できる
- 明確なエラー箇所の特定
    - デプロイプロセスがジョブとして分離されているため、いずれかのジョブが失敗した場合、どの部分に問題があったのかが一目瞭然となる
    - ログもジョブごとに分割されるため、原因調査が迅速に行える
- 失敗時の影響範囲の限定（Fail-fast）
    - あるジョブが失敗しても、依存関係にない他のジョブの実行には影響しない
    - ジョブ内の一つのステップで実行する場合、途中で失敗すると後続の無関係な処理も実行されないが、ジョブ分割によりその問題を回避できる
- （備考）単一ワークフロー内でジョブを分離する理由
    - ワークフロー自体を分割しworkflow_runで依存関係を定義する方法もあるが、設定が煩雑になりやすい
    - 本プロジェクトの規模では、単一のワークフローファイル内でジョブを分割しneedsで依存関係を管理する方が、全体の流れを把握しやすくシンプルである
    - 将来的にワークフローが過度に肥大化した場合は、その時点でワークフロー自体の分割を検討する方針とする

<br>

**2AZ構成で可用性を高めた理由**
- 1AZのみ：コストは最小だが、AZ障害で全停止。SPOFが生まれて可用性低下
- 2AZ構成：片方のAZが障害になってもサービス継続可能。SPOFの発生を抑えることができる。
- 3AZ構成：可用性は最も高いが、金銭的なコストが上がる
- 総合評価：コストと可用性のバランスを考えたときに、2AZ構成が最もベストだと考えたため

<br>

**ECRに2種類のタグ（コミットハッシュ, latest）でイメージをプッシュする理由**
- コミットハッシュタグ：不変性とトレーサビリティの確保
    - 目的
        - どのコードの状態（Gitのコミット）からビルドされたイメージなのかを、正確かつ永続的に記録する
    - メリット
        - 再現性：「特定のバージョンに戻したい」場合に、コミットハッシュタグを指定すれば、その時点のコードからビルドされたイメージを確実に取得・デプロイできる
        - デバッグ：本番環境で問題が発生した際、稼働中イメージのタグから原因となったコードを正確に特定できる
        - 監査：いつ、どのコード変更に基づいたイメージがデプロイされたかの追跡が容易になる
- latestタグ：利便性と最新版の識別
    - 目的
        - 常に「最新のビルド成功バージョン」を指す、開発者にとって分かりやすい名前を提供する
    - メリット
        - 開発・テストの容易さ：開発者が手元で試す際など、コミットハッシュを毎回調べずともlatestタグを指定するだけで最新版を利用できる
        - シンプルな継続的デプロイ：「latestタグが更新されたら自動デプロイ」といった単純なパイプラインを組むことが可能（ただし、本番環境でのこの運用は通常非推奨）

<br>

**全セキュリティグループを本リポジトリで管理する理由**
- セキュリティグループの相互依存という性質への対応
    - セキュリティグループは、リソース間の通信ルールを定義する性質上、相互に参照し合う依存関係（循環参照）が発生しやすい
    - この性質は、リソースや責務に応じてスタックを分割した場合に、解決が困難な循環参照の問題を引き起こす原因となる
- 単一スタック管理による依存関係の簡素化
    - 全てのセキュリティグループを単一スタック内で定義することで、循環参照を含む依存関係をCloudFormationが内部で解決してくれる
    - これにより、複雑なデプロイ順序やスタック間の値の受け渡しを考慮する必要がなくなり、デプロイプロセスが大幅に簡素化される
- セキュリティポリシーの一元管理
    - どのリソースがどこにアクセスできるか、というネットワークセキュリティポリシー全体を、単一のテンプレートで一元的に把握・管理できる
    - ポリシーの可視性と監査性が向上する
- 責務分担としての妥当性
    - セキュリティグループは、個々のアプリケーションに閉じたリソースではなく「リソース間の関係性を定義する」共通基盤としての性格が強い
    - そのため、ネットワーク構成や共通基盤を担当するリポジトリで一元管理するのは、役割分担としても自然な設計である

<br>

**セキュリティグループをNetworkスタックから分離した理由**
- 変更頻度とライフサイクルの違い
    - VPCやサブネットといったネットワーク基盤は一度構築すると変更頻度が低いが、SGルールはアプリケーションの追加・変更に伴い、より頻繁に更新される可能性がある
    - ライフサイクルが異なるリソース群は、スタックを分離することで管理が容易になる
- 変更時の影響範囲の限定
    - CloudFormationの更新はスタック単位で行われるため、スタックを分離することで、変更の影響範囲を限定できる
    - SGルールの変更に失敗した場合でも、影響はSecurityGroupスタック内に留まり、VPCなどの基盤インフラに意図せず影響が及ぶリスクを回避できる
- 責務の分離（関心の分離）
    - Networkスタックは「ネットワークの接続性・経路（L3レベル）」を、SecurityGroupスタックは「アクセス制御（L4レベル）」をそれぞれ担当するという、明確な責務分担が可能になる
    - それぞれのテンプレートの目的が明確になり、可読性と保守性が向上する
- スタックの管理性維持
    - Networkスタックに多数のSG定義を含めると、テンプレートが肥大化し、管理やレビューが困難になる
    - スタックを適切な粒度で分割することで、長期的な管理性を維持する

<br>

**RDSのログ設定（error, auditのみ）の採用理由**
- errorログ
    - 有効化
        - DBの安定運用とトラブルシューティングに不可欠であり、CloudWatch Logsに集約する価値が非常に高いため
- slow_query_log
    - 無効化
        - 現時点でアプリケーションの利用者が少なく、パフォーマンスへの懸念が小さいため、常時有効化する必要性は低いと判断
        - パフォーマンスチューニングが必要になった際に、一時的に有効化する運用を想定している
- general_log（一般ログ）とaudit_log（監査ログ）の比較と選択
    - 目的
        - デバッグ目的で「いつ、どのようなクエリが発行されたか」を追跡したい
    - general_logの問題点
        - 全てのSQLを記録するため最も詳細だが、ログ量が膨大になりコストとパフォーマンスへの影響が大きい
    - audit_logの採用
        - audit_logは接続情報やDML/DDLといった主要な操作を記録し、general_logよりも少ないログ量でクエリ発行の概要を把握できる
        - 「発行されたクエリの把握」というデバッグ要件は、audit_logで十分に満たせると判断
- 結論
    - コストパフォーマンスとデバッグ要件のバランスを考慮し、必須のerrorログと、費用対効果の高いauditログのみを有効にする構成が合理的であると判断
    - 将来的に要件が変化した場合は、他のログを一時的または恒久的に有効化することを検討する

<br>

**RDS拡張モニタリングを無効化した理由**
- 現状の要件に対してメリットが限定的であること
    - 本アプリケーションはポートフォリオ用途であり、利用負荷が低く、高いパフォーマンス要件もない
    - そのため、OSレベルの詳細なメトリクスを高い頻度で監視する必要性は低く、標準モニタリングで十分と判断する
- コストの最適化
    - 拡張モニタリング利用時に発生するCloudWatch Logsの追加コストを完全に回避する
- 構成と運用の簡素化
    - 拡張モニタリングを無効にすることで、専用IAMロールの作成・管理や、追加のCloudWatch Logs設定といった手間を省き、構成をシンプルに保つ
- 将来的な柔軟性の確保
    - 将来的に詳細なパフォーマンス分析が必要になった場合は、その時点で拡張モニタリングを有効化できる
    - 現時点で必須ではない機能を最初から有効にする必要はないと判断する

<br>

**Network ACLを利用せず、セキュリティグループのみで制御する理由**
- Network ACL（NACL）を追加するメリットの検討
    - 多層防御
        - SGとNACLを併用することで防御層を追加でき、SGの設定ミスをカバーする最後の砦となり得る
    - 明示的な拒否
        - SGにはない明示的な拒否ルール（例：特定のIPアドレスからのアクセスを拒否する）を定義できる
    - サブネット単位の制御
        - サブネット全体へのインバウンド・アウトバウンドトラフィックを制御する
- Network ACL（NACL）を追加するデメリット
    - 設定の複雑化
        - NACLはステートレスであり、通信の往路と復路の両方のルールを定義する必要がある
    - 管理の二重化
        - SGとNACLの両方でルールを管理する必要があり、設定変更時の手間や確認箇所が増え、運用コストが増加する
- 結論
    - 現状のアーキテクチャでは、セキュリティグループによるインスタンスレベルの制御でセキュリティ要件は十分に満たせている
    - NACLを追加することによるセキュリティ上のメリットよりも、設定の複雑化や管理コスト増加といったデメリットの方が大きいと判断する
    - そのため、よりシンプルで管理しやすいセキュリティグループのみによる制御を採用する

<br>

**ドメイン登録をIaC管理せず手動で行う理由**
- 前提となる技術的制約
    - ドメイン登録を管理するCloudFormationリソース（AWS::Route53Domains::RegisteredDomain）は、us-east-1（バージニア北部）リージョンでのみサポートされている
- IaC管理（us-east-1に専用スタックを作成）の検討
    - メリット
        - ドメイン登録プロセスがコード化され、再現性や変更追跡性が向上する
    - デメリット
        - 管理対象が複数リージョンに分散し、構成が複雑化する
        - 東京リージョンのスタックと値を連携するために、Parameter Storeなどを介したリージョン間連携の実装が必要になる
        - デプロイフローの管理も複雑になる
- 手動登録の検討
    - メリット
        - リージョンを跨ぐ複雑な構成が不要で、最もシンプルかつ迅速に目的を達成できる
        - インフラ管理を東京リージョンに集中できる
    - デメリット
        - ドメイン登録が手動オペレーションとなり、IaCの原則から逸脱する
        - 変更履歴がコードとして管理されず、ヒューマンエラーの可能性がある
- 結論
    - 今回登録するドメインは1つであり、そのためだけに別リージョンにスタックを立て、リージョン間連携を組むのは過剰な設計と判断する
    - IaC化によるメリットよりも、構成の複雑化による管理・構築コストの増加というデメリットの方が大きい
    - そのため、今回は最もシンプルな手動登録を採用し、将来ドメイン数が増加した際にIaC化を再検討する方針とする
